<section class="page-section" id="page-154">
    <div class="page-header">
        <div class="page-number">154</div>
        <div class="page-title">
            <h3>MAP si Regularizare</h3>
            <span>Capitolul 5 - Conexiunea MAP-Regularization</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/page-154.jpg"
             alt="Pagina 154" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">
        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>MAP = Log-Likelihood + Log-Prior</h4>
                <p>In formula MAP, recunoastem: log p(x | Î¸) este <strong>log-likelihood-ul standard</strong>, iar log p(Î¸) corespunde <strong>distributiei prior</strong>. De exemplu, pentru regresie liniara cu prior Gaussian N(w; 0, (1/Î»)IÂ²), termenul log-prior in ecuatia 5.79 este proportional cu Î»w<sup>T</sup>w - exact <strong>weight decay</strong>!</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Vizualizare: MAP ca Regularizare</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="key-concept">
                                <h5>Echivalenta:</h5>
                                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-top: 15px;">
                                    <div style="background: var(--bg-dark); padding: 15px; border-radius: 8px; text-align: center;">
                                        <strong>Bayesian MAP</strong>
                                        <p style="font-size: 0.9rem; margin-top: 10px;">Prior: N(0, (1/Î»)I)</p>
                                    </div>
                                    <div style="background: var(--bg-dark); padding: 15px; border-radius: 8px; text-align: center;">
                                        <strong>Frequentist</strong>
                                        <p style="font-size: 0.9rem; margin-top: 10px;">Regularizare: Î»w<sup>T</sup>w</p>
                                    </div>
                                </div>
                                <p style="text-align: center; margin-top: 15px;">â‡” Dau acelasi rezultat!</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Avantajele MAP</h4>
                <p>Ca si inferenta Bayesiana completa, MAP are avantajul de a folosi informatia adusa de prior care nu se afla in datele de antrenament. Aceasta informatie ajuta la reducerea variantei estimatului MAP (comparativ cu ML). Dar se face cu pretul unui <strong>bias crescut</strong>. Nu toate strategiile de regularizare corespund inferentei MAP Bayesiene - unii termeni de regularizare nu sunt logaritmul unei distributii de probabilitate valide.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>Referinta: Mixture of Gaussians Prior</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="reference-item">
                                <strong>Nowlan and Hinton (1992)</strong> - Termeni de regularizare mai complicati pot fi derivati folosind un mixture of Gaussians ca prior, in loc de un singur Gaussian.
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
