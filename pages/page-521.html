<section class="page-section" id="page-521">
    <div class="page-header">
        <div class="page-number">521</div>
        <div class="page-title">
            <h3>Sparsitate ca Model Generativ</h3>
            <span>Capitolul 14 - Sectiunea 14.2.1 (continuare)</span>
        </div>
    </div>
    <div class="image-container">
        <img src="book_page_jpg/page-521.jpg"
             alt="Pagina 521" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">
        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Perspectiva Maximum Likelihood</h4>
                <p>In loc sa gandim sparsitatea ca regularizator pentru copiere, putem privi antrenarea autoencoder-ului sparse ca <strong>aproximarea antrenarii unui model generativ</strong>. Avem variabile vizibile x si variabile latente h, cu distributia p(x,h) = p(h)p(x|h). Log-likelihood-ul se descompune in: <strong>log p(x) = log Î£_h p(h,x)</strong>. Autoencoderul aproximeaza aceasta suma cu un singur h foarte probabil - cel produs de encoder.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Vizualizare: Descompunerea</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="formula" style="background: var(--bg-dark); padding: 20px; border-radius: 8px;">
                                <p style="text-align: center; margin-bottom: 15px;">Descompunerea log-likelihood:</p>
                                <div style="font-family: monospace; font-size: 1.1rem; line-height: 2;">
                                    <p>log p_model(x) = log Î£_h p_model(h, x)</p>
                                    <p style="color: var(--accent);">â‰ˆ log p_model(h*, x)  unde h* = f(x)</p>
                                    <p>= log p_model(h*) + log p_model(x | h*)</p>
                                </div>
                                <div style="margin-top: 15px; display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
                                    <div style="background: var(--primary); padding: 10px; border-radius: 8px; text-align: center;">
                                        <strong>log p(h)</strong><br>
                                        <span style="font-size: 0.9rem;">â†’ sparsitate</span>
                                    </div>
                                    <div style="background: var(--secondary); padding: 10px; border-radius: 8px; text-align: center;">
                                        <strong>log p(x|h)</strong><br>
                                        <span style="font-size: 0.9rem;">â†’ reconstructie</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Prior Laplace pentru Sparsitate</h4>
                <p>Daca priorul pe h este distributia <strong>Laplace</strong>: p(h_i) = (Î»/2)exp(-Î»|h_i|), atunci log-priorul devine: <strong>Î©(h) = Î»Î£|h_i|</strong> - exact penalizarea L1! Termenul constant nu afecteaza optimizarea. Alte priori precum <strong>Student-t</strong> pot induce de asemenea sparsitate. Aceasta perspectiva ofera o motivatie diferita: antrenam aproximativ un model generativ, nu doar adaugam regularizare arbitrara.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Simulare: Distributia Laplace</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import numpy as np
import matplotlib.pyplot as plt

# Comparam Gaussian vs Laplace
x = np.linspace(-5, 5, 1000)

# Gaussian (L2 penalty)
gaussian = np.exp(-x**2 / 2) / np.sqrt(2*np.pi)

# Laplace (L1 penalty, sparsitate)
laplace = 0.5 * np.exp(-np.abs(x))

print("Laplace vs Gaussian:")
print("- Laplace: varful ascutit la 0 = incurajeaza h=0")
print("- Laplace: cozi groase = permite valori mari ocazional")
print("- Gaussian: forma rotunda = nu incurajeaza h=0")

# Log-prior:
# Laplace: log p(h) = -Î»|h| + const  â†’ L1 penalty
# Gaussian: log p(h) = -hÂ²/2ÏƒÂ² + const â†’ L2 penalty

print("\nLog-prior penalizari:")
print("  Laplace â†’ L1: Î©(h) = Î»Î£|h_i|")
print("  Gaussian â†’ L2: Î©(h) = Î»Î£h_iÂ²")
                            </div>
                        </div>
                    </div>
                </div>
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>De ce Sparsitate?</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="key-concept">
                                <h5>Avantajele Reprezentarilor Sparse</h5>
                                <ul style="margin-left: 20px;">
                                    <li><strong>Interpretabilitate:</strong> Fiecare unitate activa corespunde unei caracteristici specifice</li>
                                    <li><strong>Disentanglement:</strong> Factorii de variatie sunt separati</li>
                                    <li><strong>Eficienta:</strong> Putine calcule pentru coduri cu multe zerouri</li>
                                    <li><strong>Generalizare:</strong> Mai putin overfitting prin constrangere</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Conexiunea cu log Z si Modele Probabilistice</h4>
                <p>Cercetari timpurii (Ranzato et al., 2007-2008) au explorat conexiunea dintre penalizarea de sparsitate si termenul <strong>log Z</strong> din modelele probabilistice nedirectionate. Ideea: minimizarea log Z previne modelul sa aiba probabilitate mare peste tot, iar impunerea sparsitatii previne eroarea de reconstructie sa fie mica peste tot. Aceasta conexiune este mai mult la nivel de intuitie decat matematica stricta.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Intuitie: Sparsitate ca log Z</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="background: var(--bg-dark); padding: 20px; border-radius: 8px;">
                                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                                    <div style="background: var(--primary); padding: 15px; border-radius: 8px;">
                                        <strong>Modele Probabilistice</strong>
                                        <p style="font-size: 0.9rem; margin-top: 5px;">log p(x) = log pÌƒ(x) - log Z</p>
                                        <p style="font-size: 0.8rem; color: var(--text-secondary);">log Z penalizeaza probabilitate mare oriunde</p>
                                    </div>
                                    <div style="background: var(--secondary); padding: 15px; border-radius: 8px;">
                                        <strong>Sparse Autoencoders</strong>
                                        <p style="font-size: 0.9rem; margin-top: 5px;">L = recon + Î©(h)</p>
                                        <p style="font-size: 0.8rem; color: var(--text-secondary);">Î©(h) penalizeaza reconstructie buna oriunde</p>
                                    </div>
                                </div>
                                <p style="text-align: center; margin-top: 15px; color: var(--accent);">Ambele forteaza modelul sa fie "selectiv"</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
