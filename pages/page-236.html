<section class="page-section" id="page-236">
    <div class="page-header">
        <div class="page-number">236</div>
        <div class="page-title">
            <h3>Capitolul 6: Deep Feedforward Networks</h3>
            <span>6.5.8 Complicatii si 6.5.9 Differentiere in afara DL</span>
        </div>
    </div>
    <div class="image-container">
        <img src="book_page_jpg/page-236.jpg"
             alt="Pagina 236" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>6.5.8 Complicatii</h4>
                <p>Descrierea noastra a algoritmului de back-propagation de aici este mai simpla decat implementarile folosite in practica. Am restrans definitia unei operatii sa fie o functie care returneaza un singur tensor. Implementarile software trebuie sa suporte operatii care pot returna mai mult de un tensor. De exemplu, daca vrem sa calculam atat valoarea maxima cat si indexul valorii maxime.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Operatii cu Multiple Outputuri</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import torch

# Operatii cu MULTIPLE outputuri:
x = torch.randn(10)

# torch.max returneaza (valoare, index)
max_val, max_idx = torch.max(x, dim=0)

# torch.topk returneaza (valori, indices)
top_vals, top_idxs = torch.topk(x, k=3)

# torch.sort returneaza (valori_sortate, indices)
sorted_vals, sorted_idxs = torch.sort(x)

# In implementare, backprop trebuie sa stie
# cum sa calculeze gradientul pentru fiecare output!
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Gestionarea Memoriei</h4>
                <p>Nu am descris cum sa controlam consumul de memorie al backprop. Backprop implica adesea sumarea multor tensori impreuna. In abordarea naiva, fiecare din acesti tensori ar fi calculat separat, apoi toti ar fi adunati intr-un al doilea pas. Aceasta are un bottleneck de memorie foarte mare care poate fi evitat mentinand un singur buffer si adaugand fiecare valoare la acel buffer pe masura ce este calculata.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Optimizare Memorie</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
# NAIV (memorie mare):
grads = [compute_grad_1(), compute_grad_2(), compute_grad_3()]
total_grad = sum(grads)  # Stocheaza 3 tensori!

# OPTIMIZAT (memorie mica):
total_grad = torch.zeros_like(param)
total_grad += compute_grad_1()  # Adauga in-place
total_grad += compute_grad_2()  # Nu stocheaza separat
total_grad += compute_grad_3()

# PyTorch face aceasta optimizare automat!
# grad_table acumuleaza in-place
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Tipuri de Date si Gradienti Nedefiniti</h4>
                <p>Implementarile reale de backprop trebuie de asemenea sa gestioneze diverse tipuri de date, cum ar fi 32-bit floating point, 64-bit floating point, si valori intregi. Politica pentru gestionarea fiecaruia din aceste tipuri necesita atentie speciala in design. Unele operatii au gradienti nedefiniti, si este important sa urmarim aceste cazuri.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>Gradienti Nedefiniti</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="background: var(--bg-lighter); padding: 20px; border-radius: 12px;">
                                <h5 style="color: var(--warning); margin-bottom: 15px;">Exemple de gradienti problematici:</h5>
                                <ul style="font-size: 0.9rem;">
                                    <li style="margin-bottom: 10px;"><strong>Division by zero:</strong> 1/x la x=0</li>
                                    <li style="margin-bottom: 10px;"><strong>Log of zero:</strong> log(x) la x=0</li>
                                    <li style="margin-bottom: 10px;"><strong>Sqrt of zero:</strong> sqrt(x) la x=0</li>
                                    <li style="margin-bottom: 10px;"><strong>Argmax:</strong> nu are gradient definit</li>
                                </ul>
                            </div>
                            <div class="code-block" style="margin-top: 15px;">
import torch

# Gradienti problematici:
x = torch.tensor([0.0], requires_grad=True)

# log(0) -> -inf, gradient -> inf
y = torch.log(x + 1e-8)  # Adauga epsilon!

# sqrt(0) -> gradient infinit
z = torch.sqrt(x + 1e-8)  # Adauga epsilon!

# argmax nu are gradient
idx = torch.argmax(x)  # Nu propaga gradient!
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>6.5.9 Diferentierea in afara Comunitatii Deep Learning</h4>
                <p>Comunitatea de deep learning a fost oarecum izolata de comunitatea mai larga de computer science si si-a dezvoltat propriile atitudini culturale privind modul de a efectua diferentiere. Mai general, domeniul automatic differentiation se ocupa de cum sa calculeze derivate algoritmic.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>Automatic Differentiation</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="key-concept">
                                <strong>Backprop = Reverse Mode Automatic Differentiation</strong>
                                <p style="margin-top: 10px;">Backprop este un caz special al unei clase mai largi de tehnici numite reverse mode accumulation. Alte abordari evalueaza subexpresiile regulii lantului in ordini diferite.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>
