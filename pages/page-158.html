<section class="page-section" id="page-158">
    <div class="page-header">
        <div class="page-number">158</div>
        <div class="page-title">
            <h3>K-Nearest Neighbors</h3>
            <span>Capitolul 5 - Sectiunea 5.7.3</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/page-158.jpg"
             alt="Pagina 158" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">
        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Limitarile Kernel Machines</h4>
                <p>Kernel machines sufera de cost computational ridicat la antrenare cand datasetul este mare (vom revedea in sectiunea 5.9). De asemenea, kernel machines cu kernels generice au dificultati la generalizare - vom explica de ce in sectiunea 5.11. <strong>Deep learning</strong> a fost conceput pentru a depasi aceste limitari. Rena≈üterea deep learning a inceput cand <strong>Hinton et al. (2006)</strong> au demonstrat ca o retea neurala poate depasi SVM cu kernel RBF pe benchmark-ul MNIST.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">üìö</div>
                        <span>Referinta: Deep Learning Renaissance</span>
                        <span class="arrow">‚ñ∂</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="reference-item">
                                <strong>Hinton et al. (2006)</strong> - A fast learning algorithm for deep belief nets. Acest paper a marcat inceputul erei moderne a deep learning, demonstrand ca retelele neurale profunde pot fi antrenate eficient.
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>5.7.3 Other Simple Supervised Learning Algorithms</h4>
                <p>Am intalnit deja un algoritm supervised non-probabilistic: <strong>nearest neighbor regression</strong>. Mai general, <strong>k-nearest neighbors</strong> este o familie de tehnici pentru clasificare sau regresie. Ca algoritm <strong>non-parametric</strong>, k-NN nu este restrictionat la un numar fix de parametri - ganditi-va la el ca neavand parametri, ci implementand o functie simpla a datelor de antrenare. La test time, pentru input x, gasim k-nearest neighbors in datele de training X si returnam media y-urilor corespunzatoare.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">üéÆ</div>
                        <span>Simulare: k-NN Clasificare</span>
                        <span class="arrow">‚ñ∂</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# Date de training
X_train = np.array([[1, 1], [1, 2], [2, 1], [5, 5], [5, 6], [6, 5]])
y_train = np.array([0, 0, 0, 1, 1, 1])

# k-NN cu diferite valori de k
for k in [1, 3, 5]:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    # Test point
    X_test = np.array([[3, 3]])
    pred = knn.predict(X_test)

    print(f"k={k}: Predictie pentru [3,3] = clasa {pred[0]}")

print()
print("=> k mic = mai sensibil la noise")
print("=> k mare = decision boundary mai smooth")
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Capacitatea k-NN</h4>
                <p>Ca algoritm non-parametric, k-NN poate atinge <strong>capacitate foarte mare</strong>. Consideram un task de clasificare multiclass cu loss 0-1. In aceasta setare, <strong>1-nearest neighbor converge la dublul Bayes error</strong> pe masura ce numarul de exemple de training ‚Üí ‚àû. Eroarea in exces fata de Bayes error provine din alegerea aleatorie a unui singur neighbor cand exista tie-uri intre vecini la distante egale. Cu date infinite, toate punctele de test au infinit de multi vecini la distanta zero. Daca folosim toti vecinii pentru vot (in loc sa alegem aleator), procedura converge la <strong>Bayes error rate</strong>!</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">‚ú®</div>
                        <span>Vizualizare: Convergenta k-NN</span>
                        <span class="arrow">‚ñ∂</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="key-concept">
                                <h5>Proprietati Asimptotice k-NN</h5>
                                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin-top: 15px;">
                                    <div style="background: var(--bg-dark); padding: 15px; border-radius: 8px; text-align: center;">
                                        <strong>1-NN</strong>
                                        <p style="font-size: 0.9rem; margin-top: 10px;">Error ‚Üí 2 √ó Bayes Error</p>
                                        <p style="font-size: 0.8rem; color: var(--text-secondary);">(cand n ‚Üí ‚àû)</p>
                                    </div>
                                    <div style="background: var(--bg-dark); padding: 15px; border-radius: 8px; text-align: center;">
                                        <strong>k-NN cu vot</strong>
                                        <p style="font-size: 0.9rem; margin-top: 10px;">Error ‚Üí Bayes Error</p>
                                        <p style="font-size: 0.8rem; color: var(--text-secondary);">(cand n ‚Üí ‚àû)</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Dezavantajele k-NN</h4>
                <p>Capacitatea mare a k-NN ii permite sa obtina acuratete ridicata cu un training set mare. Totusi, face asta cu <strong>cost computational ridicat</strong> si poate <strong>generaliza foarte prost</strong> cu un training set mic. O slabiciune a k-NN este ca <strong>nu poate invata ca o feature este mai discriminativa decat alta</strong>. De exemplu, pentru o problema de regresie cu x ‚àà R<sup>100</sup> din Gaussian izotrop, dar unde doar x‚ÇÅ este relevanta pentru output (y = x‚ÇÅ), nearest neighbor va fi dominat de cele 99 de features irelevante!</p>
            </div>
        </div>
    </div>
</section>
