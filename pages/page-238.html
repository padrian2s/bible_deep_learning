<section class="page-section" id="page-238">
    <div class="page-header">
        <div class="page-number">238</div>
        <div class="page-title">
            <h3>Capitolul 6: Deep Feedforward Networks</h3>
            <span>6.5.9 Forward Mode vs Backward Mode</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT)-page-238.jpg"
             alt="Pagina 238" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Forward Mode Accumulation</h4>
                <p>Cand numarul de outputuri al grafului este mai mare decat numarul de inputuri, este uneori preferabil sa folosim o alta forma de automatic differentiation numita forward mode accumulation. Forward mode computation a fost propusa pentru obtinerea calculului real-time al gradientilor in retele recurente (Williams si Zipser, 1989). Aceasta evita nevoia de a stoca valorile si gradientii pentru intregul graf.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Ecuatia 6.58: Analogia Matriceala</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="key-concept">
                                <strong>Analogia cu inmultirea matriceala:</strong>
                                <div class="formula" style="margin-top: 15px; font-size: 1.3rem; text-align: center;">
                                    ABCD
                                </div>
                                <p style="margin-top: 15px; font-size: 0.9rem; color: var(--text-secondary);">Matricile pot fi gandite ca Jacobieni. Ordinea inmultirii determina eficienta!</p>
                            </div>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 15px;">
                                <div style="background: var(--bg-lighter); padding: 15px; border-radius: 8px;">
                                    <strong style="color: var(--primary);">Backward Mode (dreapta-stanga)</strong>
                                    <p style="font-size: 0.85rem; margin-top: 10px;">((AB)C)D - bun cand D are multe randuri (multe outputuri)</p>
                                    <p style="font-size: 0.85rem; color: var(--success); margin-top: 5px;">Folosit in deep learning!</p>
                                </div>
                                <div style="background: var(--bg-lighter); padding: 15px; border-radius: 8px;">
                                    <strong style="color: var(--accent);">Forward Mode (stanga-dreapta)</strong>
                                    <p style="font-size: 0.85rem; margin-top: 10px;">A(B(CD)) - bun cand A are putine randuri (multe inputuri)</p>
                                    <p style="font-size: 0.85rem; color: var(--text-secondary); margin-top: 5px;">Folosit rar in DL</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>De ce Backward Mode pentru Deep Learning?</h4>
                <p>Daca D este un vector coloana in timp ce A are multe randuri, aceasta corespunde unui graf cu un singur output si multe inputuri, iar incepand inmultirile de la sfarsit si mergand inapoi necesita doar produse matrice-vector. Aceasta corespunde modului backward. In schimb, daca A are mai putine randuri decat D are coloane, este mai ieftin sa rulam inmultirile stanga-dreapta, corespunzand modului forward.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Exemplu Numeric</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import numpy as np

# Retea neurala tipica:
# Input: 1000 features
# Hidden: 500 neurons
# Output: 1 (loss scalar)

# Jacobieni:
# J1: (500, 1000) - input -> hidden
# J2: (1, 500)    - hidden -> output

# BACKWARD MODE (dreapta-stanga):
# Step 1: J2 @ gradient (1, 500) @ (500,) -> (1,)
# Step 2: J1^T @ result (1000, 500) @ (500,) -> (1000,)
# Cost: O(500) + O(500*1000) = O(500*1000)

# FORWARD MODE (stanga-dreapta):
# Step 1: J1 @ input_grad (500, 1000) @ (1000,) -> (500,)
# Pentru fiecare input separat!
# Cost: O(1000 * 500 * 1000) - MULT MAI SCUMP!

# Concluzie: Backward mode e ideal pentru:
# - Multi parametri (milioane)
# - Un singur output (loss scalar)
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Biblioteci Specializate vs Generale</h4>
                <p>In multe comunitati din afara machine learning, este mai comun sa implementam software de diferentiere care actioneaza direct pe cod in limbaje de programare traditionale, precum Python sau C. In comunitatea deep learning, grafurile computationale sunt de obicei reprezentate explicit de structuri de date. Abordarea specializata are avantajul de a permite reguli bprop customizate pentru fiecare operatie.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>Abordari de Diferentiere</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="background: var(--bg-lighter); padding: 20px; border-radius: 12px;">
                                <table style="width: 100%; border-collapse: collapse; font-size: 0.85rem;">
                                    <tr style="background: var(--primary);">
                                        <th style="padding: 10px;">Abordare</th>
                                        <th style="padding: 10px;">Exemple</th>
                                        <th style="padding: 10px;">Pro/Con</th>
                                    </tr>
                                    <tr style="background: var(--bg-dark);">
                                        <td style="padding: 10px;">Symbolic (explicit)</td>
                                        <td style="padding: 10px;">PyTorch, TensorFlow, JAX</td>
                                        <td style="padding: 10px;">Optimizat pentru DL</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 10px;">Source-to-source</td>
                                        <td style="padding: 10px;">Tapenade, ADIFOR</td>
                                        <td style="padding: 10px;">General-purpose</td>
                                    </tr>
                                    <tr style="background: var(--bg-dark);">
                                        <td style="padding: 10px;">Operator overloading</td>
                                        <td style="padding: 10px;">Autograd, JAX</td>
                                        <td style="padding: 10px;">Flexibil</td>
                                    </tr>
                                </table>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>
