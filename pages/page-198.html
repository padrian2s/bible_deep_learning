<section class="page-section" id="page-198">
    <div class="page-header">
        <div class="page-number">198</div>
        <div class="page-title">
            <h3>Capitolul 6: Deep Feedforward Networks</h3>
            <span>6.2.2.2 Sigmoid: Logit si Softplus</span>
        </div>
    </div>
    <div class="image-container">
        <img src="book_page_jpg/page-198.jpg"
             alt="Pagina 198" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Structura Unitatii Sigmoid</h4>
                <p>Putem gandi unitatea sigmoid output ca avand doua componente. Prima parte calculeaza z = wáµ€h + b folosind un strat liniar. A doua parte foloseste functia de activare sigmoid pentru a converti z intr-o probabilitate. Variabila z definind o distributie peste variabile binare se numeste logit.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Logit: De la Probabilitate la Log-Odds</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="formula" style="background: var(--bg-lighter); padding: 15px; border-radius: 8px; margin-bottom: 15px;">
                                <p><strong>Logit = log-odds:</strong></p>
                                <p style="margin-top: 10px;">z = log(P(y=1) / P(y=0)) = log(p / (1-p))</p>
                                <p style="margin-top: 10px;"><strong>Sigmoid = inversa logit:</strong></p>
                                <p style="margin-top: 10px;">p = Ïƒ(z) = 1 / (1 + e<sup>-z</sup>)</p>
                            </div>
                            <div class="code-block">
import torch
import torch.nn.functional as F

# Logit: numarul real pe care reteaua il produce
z = torch.tensor([[-2.0], [0.0], [2.0]])

# Sigmoid transforma logit in probabilitate
prob = torch.sigmoid(z)
print(prob)  # [[0.12], [0.50], [0.88]]

# Logit e neconstrans (-inf, +inf)
# Probabilitatea e in (0, 1)

# In practica: reteaua outputeaza logits,
# functia de loss aplica sigmoid intern
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Derivarea Sigmoid din Distributia Bernoulli</h4>
                <p>Sigmoid-ul poate fi motivat construind o distributie de probabilitate nenormalizata P~(y) care nu sumeaza la 1, apoi normalizand. Daca presupunem ca log-probabilitatile nenormalizate sunt liniare in y si z, exponentiind si normalizand obtinem exact distributia Bernoulli controlata de sigmoid: P(y) = Ïƒ((2y-1)z).</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Derivare pas cu pas</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="background: var(--bg-lighter); padding: 20px; border-radius: 12px;">
                                <div style="display: grid; gap: 15px;">
                                    <div style="padding: 10px; background: var(--bg-dark); border-radius: 8px;">
                                        <strong>Pas 1:</strong> log P~(y) = yz
                                    </div>
                                    <div style="padding: 10px; background: var(--bg-dark); border-radius: 8px;">
                                        <strong>Pas 2:</strong> P~(y) = exp(yz)
                                    </div>
                                    <div style="padding: 10px; background: var(--bg-dark); border-radius: 8px;">
                                        <strong>Pas 3:</strong> P(y) = exp(yz) / Î£ exp(y'z)
                                    </div>
                                    <div style="padding: 10px; background: linear-gradient(90deg, rgba(16, 185, 129, 0.2) 0%, var(--bg-dark) 20%); border-radius: 8px; box-shadow: 0 0 15px rgba(16, 185, 129, 0.1);">
                                        <strong>Rezultat:</strong> P(y) = Ïƒ((2y-1)z)
                                    </div>
                                </div>
                                <p style="margin-top: 15px; color: var(--text-secondary); font-size: 0.9rem;">Pentru y=1: P(1) = Ïƒ(z). Pentru y=0: P(0) = Ïƒ(-z) = 1-Ïƒ(z).</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Functia de Cost cu Softplus</h4>
                <p>Functia de cost pentru maximum likelihood cu sigmoid este J(Î¸) = -log P(y | x) = Î¶((1-2y)z), unde Î¶ este functia softplus: Î¶(x) = log(1 + exp(x)). Rescriind loss-ul in termeni de softplus, vedem ca satureaza doar cand (1-2y)z este foarte negativ - adica doar cand modelul deja are raspunsul corect!</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Functia Softplus</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import torch
import torch.nn.functional as F

# Softplus: versiune "smooth" a ReLU
def softplus(x):
    return torch.log(1 + torch.exp(x))

# echivalent cu F.softplus(x)

x = torch.linspace(-5, 5, 100)
sp = F.softplus(x)

# Proprietati:
# softplus(x) â‰ˆ 0 pentru x << 0
# softplus(x) â‰ˆ x pentru x >> 0
# softplus(0) = log(2) â‰ˆ 0.693

# BCE Loss in termeni de softplus:
# Loss = softplus((1 - 2*y) * z)

# Daca y=1 si z >> 0 (corect): loss â‰ˆ 0
# Daca y=1 si z << 0 (gresit): loss â‰ˆ |z| (mare!)
# Gradient puternic cand greseste!
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>
