<section class="page-section" id="page-372">
    <div class="page-header">
        <div class="page-number">372</div>
        <div class="page-title">
            <h3>Gradientii pentru CNN</h3>
            <span>Formule de Backpropagation</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT)-page-372.jpg"
             alt="Pagina 372" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Gradientul fata de Kernel (Weight Update)</h4>
                <p>Pentru a actualiza ponderile kernelului, calculam: <span class="formula">g(G,V,s)[i,j,k,l] = âˆ‚J/âˆ‚K[i,j,k,l] = Î£_{m,n} G[s,m,n] Ã— V[j,(m-1)Ã—s+k,(n-1)Ã—s+l]</span>. Aceasta e o convolutie intre gradinetul output-ului (G) si input-ul original (V). Intuitiv: kernelul trebuie ajustat in directia care reduce eroarea, proportional cu activarea inputului.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Simulare: Gradient fata de Kernel</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import torch

# Exemplu simplificat: conv 1D, batch=1
# Input x, kernel k, output z = conv(x, k)
# Loss L, gradient dL/dz = grad_output

x = torch.tensor([[1., 2., 3., 4., 5.]])  # [1, 5]
k = torch.tensor([[1., 0., -1.]])  # [1, 3]

# Forward: z = x * k (cross-correlation)
# z = [1*1 + 2*0 + 3*(-1), 2*1 + 3*0 + 4*(-1), ...]
#   = [-2, -2, -2]

# Backward: presupunem grad_output = [1, 1, 1]
grad_output = torch.tensor([[1., 1., 1.]])

# Gradient fata de k:
# dk[i] = sum_j (grad_output[j] * x[j+i])
# dk[0] = 1*1 + 1*2 + 1*3 = 6
# dk[1] = 1*2 + 1*3 + 1*4 = 9
# dk[2] = 1*3 + 1*4 + 1*5 = 12
grad_k = torch.tensor([[6., 9., 12.]])
print("Gradient kernel:", grad_k)
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Gradientul fata de Input (Backprop)</h4>
                <p>Pentru a propaga eroarea inapoi: <span class="formula">h(K,G,s)[i,j,k] = âˆ‚J/âˆ‚V[i,j,k] = Î£_{l,m,n,p,q} K[q,i,m,p] Ã— G[q,l,n]</span> unde sumarea e peste pozitiile valide. Aceasta este <strong>transpose convolution</strong> - aplicam kernelul "invers" pe gradient pentru a obtine contributia fiecarui pixel input la eroare.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Simulare: ConvTranspose2d</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import torch
import torch.nn as nn

# ConvTranspose2d - folosit explicit in generatoare

# Conv normal: reduce dimensiunea
conv = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
x = torch.randn(1, 64, 32, 32)
y = conv(x)
print("Conv:", x.shape, "->", y.shape)  # 32x32 -> 16x16

# ConvTranspose: mareste dimensiunea (invers)
conv_t = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)
z = conv_t(y)
print("ConvT:", y.shape, "->", z.shape)  # 16x16 -> 32x32

# Folosit in:
# - Decoder-e de autoencoders
# - Generator de GAN
# - Upsampling in segmentare semantica
                            </div>
                        </div>
                    </div>
                </div>
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>Terminologie: Deconvolution?</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="key-concept">
                                <strong>Atentie la terminologie!</strong><br><br>
                                "Deconvolution" este termenul gresit dar popular. Matematic, deconvolutia e inversa convolutiei (rezolva x din y = x * k). Transpose convolution NU face asta - doar transpune matricea de ponderi.<br><br>
                                Termeni corecti: <strong>transpose convolution</strong>, <strong>fractionally-strided convolution</strong>, sau <strong>upconvolution</strong>.
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>
