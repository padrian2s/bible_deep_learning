<section class="page-section" id="page-148">
    <div class="page-header">
        <div class="page-number">148</div>
        <div class="page-title">
            <h3>Conditional Log-Likelihood si MSE</h3>
            <span>Capitolul 5 - Sectiunea 5.5.1</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT)-page-148.jpg"
             alt="Pagina 148" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">
        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>5.5.1 Conditional Log-Likelihood</h4>
                <p>Estimatorul maximum likelihood se generalizeaza natural la cazul in care vrem sa estimam o <strong>probabilitate conditionata</strong> P(y | x; Î¸) pentru a prezice y dat fiind x. Aceasta este situatia cea mai comuna in supervised learning. Daca X reprezinta toate input-urile si Y toate target-urile:</p>
                <div class="formula" style="text-align: center; font-size: 1.3rem; margin: 15px 0; padding: 15px; background: linear-gradient(135deg, var(--primary), var(--secondary)); border-radius: 8px;">
                    Î¸<sub>ML</sub> = argmax<sub>Î¸</sub> P(Y | X; Î¸)
                </div>
                <p>Pentru exemple i.i.d., aceasta se descompune in: Î¸<sub>ML</sub> = argmax<sub>Î¸</sub> Î£áµ¢ log P(y<sup>(i)</sup> | x<sup>(i)</sup>; Î¸)</p>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Linear Regression ca Maximum Likelihood</h4>
                <p>Regresia liniara poate fi justificata ca procedura MLE. In loc sa producem o singura predictie Å·, gandim modelul ca producand o <strong>distributie conditionata</strong> p(y | x). Cu un training set infinit de mare, am vedea mai multe valori y pentru acelasi x - modelul trebuie sa le potriveasca pe toate. Definim:</p>
                <div class="formula" style="text-align: center; font-size: 1.2rem; margin: 15px 0; padding: 15px; background: var(--bg-dark); border-radius: 8px;">
                    p(y | x) = N(y; Å·(x; w), ÏƒÂ²)
                </div>
                <p>unde Å·(x; w) este media Gaussianului (predictia noastra), iar ÏƒÂ² este varianta (un hiperparameter fix).</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Simulare: Probabilistic Regression</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import numpy as np
from scipy import stats

# Model probabilistic: p(y|x) = N(y; w*x, sigma^2)
w = 2.0
sigma = 0.5

def log_likelihood(x, y):
    """Log-likelihood pentru un punct (x, y)"""
    y_pred = w * x
    return stats.norm.logpdf(y, loc=y_pred, scale=sigma)

# Date
X = np.array([1, 2, 3])
y = np.array([2.1, 3.9, 6.2])

# Log-likelihood total
total_ll = sum(log_likelihood(xi, yi) for xi, yi in zip(X, y))

print("Model: p(y|x) = N(y; w*x, ÏƒÂ²)")
print(f"w = {w}, Ïƒ = {sigma}")
print()
print("Log-likelihoods individuale:")
for xi, yi in zip(X, y):
    ll = log_likelihood(xi, yi)
    print(f"  x={xi}, y={yi:.1f}: log p(y|x) = {ll:.3f}")
print(f"\nTotal log-likelihood: {total_ll:.3f}")
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
