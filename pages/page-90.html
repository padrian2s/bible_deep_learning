        <section class="page-section" id="page-90">
            <div class="page-header">
                <div class="page-number">90</div>
                <div class="page-title">
                    <h3>Entropia Binara si Cross-Entropy</h3>
                    <span>3.14 Modele Probabilistice Structurate (Intro)</span>
                </div>
            </div>
            <div class="image-container">
                <img src="book_page_jpg/page-090.jpg"
                     alt="Pagina 90" class="page-image" onclick="zoomImage(this)">
            </div>
            <div class="explanation-content">

                <div class="interactive-paragraph">
                    <div class="paragraph-main" onclick="toggleParagraph(this)">
                        <h4>Figura 3.5: Entropia Binara</h4>
                        <p>Graficul arata entropia lui Bernoulli(p) in functie de p. Formula: H(p) = -(p)log(p) - (1-p)log(1-p). Entropia e <strong>minima</strong> (‚âà 0) cand p ‚âà 0 sau p ‚âà 1 (aproape deterministic). Entropia e <strong>maxima</strong> la p = 0.5 (maxim de incertitudine - moneda corecta). Valoarea maxima in nats e ln(2) ‚âà 0.693.</p>
                    </div>
                    <div class="expandable-sections">
                        <div class="section-tab">
                            <div class="section-header" onclick="toggleSection(this)">
                                <div class="section-icon simulation">üéÆ</div>
                                <span>Cod: Entropia binara</span>
                                <span class="arrow">‚ñ∂</span>
                            </div>
                            <div class="section-content">
                                <div class="section-body">
                                    <div class="code-block">
import numpy as np

def binary_entropy(p):
    """H(p) = -p*log(p) - (1-p)*log(1-p)"""
    if p <= 0 or p >= 1:
        return 0.0
    return -p * np.log(p) - (1-p) * np.log(1-p)

# Generam curba
ps = np.linspace(0.01, 0.99, 50)
hs = [binary_entropy(p) for p in ps]

print("Entropia binara:")
print(f"  H(0.01) = {binary_entropy(0.01):.4f} (aproape determinist)")
print(f"  H(0.5) = {binary_entropy(0.5):.4f} (maxim incertitudine)")
print(f"  H(0.99) = {binary_entropy(0.99):.4f} (aproape determinist)")
print(f"\n  Maximum teoretic: ln(2) = {np.log(2):.4f}")

# Conexiunea cu cross-entropy loss:
# BCE = -[y*log(p) + (1-y)*log(1-p)]
# Cand y=1: BCE = -log(p)
# Cand y=0: BCE = -log(1-p)
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="interactive-paragraph">
                    <div class="paragraph-main" onclick="toggleParagraph(this)">
                        <h4>Cross-Entropy</h4>
                        <p>O cantitate strans legata de KL divergence e <strong>cross-entropy</strong>: H(P, Q) = H(P) + D_KL(P||Q) = -ùîº‚Çì~P[log Q(x)]. Minimizarea cross-entropy fata de Q e echivalenta cu minimizarea KL divergence (pentru ca H(P) e constant relativ la Q). De aceea <strong>cross-entropy loss</strong> e atat de folosita in clasificare!</p>
                    </div>
                    <div class="expandable-sections">
                        <div class="section-tab">
                            <div class="section-header" onclick="toggleSection(this)">
                                <div class="section-icon animation">‚ú®</div>
                                <span>Vizualizare: H, KL, Cross-Entropy</span>
                                <span class="arrow">‚ñ∂</span>
                            </div>
                            <div class="section-content">
                                <div class="section-body">
                                    <div class="formula" style="text-align: center; font-size: 1.1rem; margin-bottom: 15px; padding: 15px; background: var(--bg-lighter); border-radius: 8px;">
                                        H(P, Q) = H(P) + D_KL(P || Q)
                                    </div>
                                    <div style="display: grid; grid-template-columns: 1fr auto 1fr auto 1fr; gap: 10px; align-items: center; text-align: center;">
                                        <div style="background: var(--bg-lighter); padding: 10px; border-radius: 8px;">
                                            <strong style="color: var(--secondary);">Cross-Entropy</strong>
                                            <p style="font-size: 0.75rem; color: var(--text-secondary);">H(P, Q)</p>
                                        </div>
                                        <span>=</span>
                                        <div style="background: var(--bg-lighter); padding: 10px; border-radius: 8px;">
                                            <strong style="color: var(--accent);">Entropy</strong>
                                            <p style="font-size: 0.75rem; color: var(--text-secondary);">H(P)</p>
                                        </div>
                                        <span>+</span>
                                        <div style="background: var(--bg-lighter); padding: 10px; border-radius: 8px;">
                                            <strong style="color: var(--warning);">KL Divergence</strong>
                                            <p style="font-size: 0.75rem; color: var(--text-secondary);">D_KL(P||Q)</p>
                                        </div>
                                    </div>
                                    <p style="margin-top: 15px; font-size: 0.85rem; color: var(--text-secondary); text-align: center;">
                                        H(P) e constant ‚Üí min H(P,Q) ‚ü∫ min D_KL(P||Q)
                                    </p>
                                </div>
                            </div>
                        </div>
                        <div class="section-tab">
                            <div class="section-header" onclick="toggleSection(this)">
                                <div class="section-icon simulation">üéÆ</div>
                                <span>Cod: Cross-entropy loss</span>
                                <span class="arrow">‚ñ∂</span>
                            </div>
                            <div class="section-content">
                                <div class="section-body">
                                    <div class="code-block">
import numpy as np

def cross_entropy(p_true, q_pred):
    """H(P, Q) = -sum(P * log(Q))"""
    p_true, q_pred = np.array(p_true), np.array(q_pred)
    # Clip pentru stabilitate numerica
    q_pred = np.clip(q_pred, 1e-15, 1 - 1e-15)
    return -np.sum(p_true * np.log(q_pred))

# Clasificare: y_true e one-hot
y_true = [1, 0, 0]  # Clasa 0
y_pred = [0.7, 0.2, 0.1]  # Predictia modelului (softmax output)

loss = cross_entropy(y_true, y_pred)
print(f"Cross-entropy loss: {loss:.4f}")

# Predictie mai buna ‚Üí loss mai mic
y_pred_better = [0.9, 0.05, 0.05]
loss_better = cross_entropy(y_true, y_pred_better)
print(f"Cu predictie mai buna: {loss_better:.4f}")

# Predictie perfecta
y_pred_perfect = [1.0, 0.0, 0.0]
loss_perfect = cross_entropy(y_true, y_pred_perfect)
print(f"Predictie perfecta: {loss_perfect:.4f}")

# Aceasta e exact ce calculeaza PyTorch nn.CrossEntropyLoss!
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="interactive-paragraph">
                    <div class="paragraph-main" onclick="toggleParagraph(this)">
                        <h4>3.14 Modele Probabilistice Structurate - Introducere</h4>
                        <p>Algoritmii ML implica adesea distributii de probabilitate peste un numar FOARTE mare de variabile. Descrierea distributiei complete poate fi ineficienta - numarul de parametri creste exponential! Solutia: <strong>factorizam</strong> distributia in produs de distributii mai simple. Aceasta factorizare se poate descrie folosind <strong>grafuri</strong>.</p>
                    </div>
                    <div class="expandable-sections">
                        <div class="section-tab">
                            <div class="section-header" onclick="toggleSection(this)">
                                <div class="section-icon reference">üìö</div>
                                <span>De ce factorizare?</span>
                                <span class="arrow">‚ñ∂</span>
                            </div>
                            <div class="section-content">
                                <div class="section-body">
                                    <div class="key-concept">
                                        <strong>Problema:</strong> Pentru n variabile binare, distributia completa are 2‚Åø - 1 parametri!
                                        <p style="margin-top: 10px;">n = 100 variabile ‚Üí ~10¬≥‚Å∞ parametri (imposibil!)</p>
                                        <p style="margin-top: 10px;"><strong>Solutia:</strong> Presupunem independente conditionate si factorizam:</p>
                                        <div class="formula" style="margin: 10px 0;">p(a, b, c) = p(a) √ó p(b|a) √ó p(c|b)</div>
                                        <p style="color: var(--success);">Daca fiecare factor are maxim k variabile ‚Üí O(n √ó 2·µè) parametri!</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </section>
