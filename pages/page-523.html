<section class="page-section" id="page-523">
    <div class="page-header">
        <div class="page-number">523</div>
        <div class="page-title">
            <h3>Contractive Autoencoders & Adancime</h3>
            <span>Capitolul 14 - Sectiunile 14.2.3 si 14.3</span>
        </div>
    </div>
    <div class="image-container">
        <img src="book_page_jpg/page-523.jpg"
             alt="Pagina 523" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">
        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>14.2.3 Regularizare prin Penalizarea Derivatelor</h4>
                <p>O alta strategie de regularizare este sa folosim o penalizare Î© diferita: <strong>Î©(h,x) = Î»Î£||âˆ‡_x h_i||Â²</strong> - suma patratelor normelor gradientilor fiecarei unitati h_i in raport cu inputul x. Aceasta forteaza modelul sa invete o functie care <strong>nu se schimba mult cand x se schimba putin</strong>. Un autoencoder regularizat astfel se numeste <strong>contractive autoencoder</strong> (CAE).</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Simulare: Jacobian Penalty</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import torch
import torch.nn as nn

class ContractiveAutoencoder(nn.Module):
    def __init__(self, input_dim, code_dim, lambda_cae=1e-4):
        super().__init__()
        self.lambda_cae = lambda_cae
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.Sigmoid(),  # Nevoie de functie diferentiabila
            nn.Linear(256, code_dim),
            nn.Sigmoid()
        )
        self.decoder = nn.Sequential(
            nn.Linear(code_dim, 256),
            nn.Sigmoid(),
            nn.Linear(256, input_dim)
        )

    def forward(self, x):
        h = self.encoder(x)
        r = self.decoder(h)
        return r, h

    def contractive_loss(self, x, h):
        # Calculeaza ||âˆ‚h/âˆ‚x||Â² (norma Frobenius a Jacobianului)
        h.backward(torch.ones_like(h), retain_graph=True)
        jacobian_norm = x.grad.pow(2).sum()
        return self.lambda_cae * jacobian_norm

# CAE forteaza h sa fie STABIL la mici perturbatii ale lui x
                            </div>
                        </div>
                    </div>
                </div>
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>Conexiuni Teoretice</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="key-concept">
                                <p>CAE are conexiuni teoretice cu:</p>
                                <ul style="margin-left: 20px;">
                                    <li><strong>Denoising AE:</strong> Ambele invata reprezentari stabile</li>
                                    <li><strong>Manifold learning:</strong> Reprezinta datele pe varietati</li>
                                    <li><strong>Modele probabilistice:</strong> Score matching</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>14.3 Puterea Reprezentationala si Adancimea</h4>
                <p>Autoencoderele sunt adesea antrenate cu un singur strat encoder si decoder. Insa folosirea de <strong>encodere si decodere adanci</strong> ofera multe avantaje. Teorema aproximatorului universal garanteaza ca o retea cu un strat ascuns poate reprezenta orice functie, dar maparea de la input la cod poate fi shallow. Un autoencoder <strong>adanc</strong> poate impune constrangeri arbitrare (cum ar fi sparsitatea) si poate reduce exponential costul computational si cantitatea de date necesare.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Vizualizare: Shallow vs Deep</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: var(--bg-dark); padding: 15px; border-radius: 8px; text-align: center;">
                                    <h5>Shallow AE</h5>
                                    <div style="margin: 10px 0;">x â†’ [h] â†’ r</div>
                                    <p style="font-size: 0.9rem; color: var(--text-secondary);">Un singur strat ascuns</p>
                                </div>
                                <div style="background: var(--bg-dark); padding: 15px; border-radius: 8px; text-align: center;">
                                    <h5>Deep AE</h5>
                                    <div style="margin: 10px 0;">x â†’ [h1] â†’ [h2] â†’ [h3] â†’ r</div>
                                    <p style="font-size: 0.9rem; color: var(--success);">Mai putini parametri, compresie mai buna!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Simulare: Deep vs Shallow Compression</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import torch.nn as nn

# Shallow autoencoder pentru MNIST
class ShallowAE(nn.Module):
    def __init__(self):
        super().__init__()
        # Un singur strat hidden - are nevoie de MULTE unitati
        self.encoder = nn.Sequential(
            nn.Linear(784, 32)  # Direct 784 â†’ 32
        )
        self.decoder = nn.Sequential(
            nn.Linear(32, 784)
        )
    # Parametri: 784*32 + 32*784 = 50,176

# Deep autoencoder - mult mai eficient
class DeepAE(nn.Module):
    def __init__(self):
        super().__init__()
        # Reducere graduala - fiecare strat invata abstractii
        self.encoder = nn.Sequential(
            nn.Linear(784, 256), nn.ReLU(),
            nn.Linear(256, 64), nn.ReLU(),
            nn.Linear(64, 32)  # Final: 32 dim
        )
        self.decoder = nn.Sequential(
            nn.Linear(32, 64), nn.ReLU(),
            nn.Linear(64, 256), nn.ReLU(),
            nn.Linear(256, 784)
        )
    # Parametri: 784*256 + 256*64 + 64*32 + ... = ~250K
    # DAR: reconstruieste MULT mai bine pentru aceeasi dim cod!

# Hinton & Salakhutdinov (2006): Deep AE >> Shallow AE >> PCA
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
