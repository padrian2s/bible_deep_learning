<section class="page-section" id="page-278">
    <div class="page-header">
        <div class="page-number">278</div>
        <div class="page-title">
            <h3>7.12 Weight Scaling Inference Rule</h3>
            <span>Inferenta Eficienta cu Scalarea Weights</span>
        </div>
    </div>
    <div class="image-container">
        <img src="book_page_jpg/page-278.jpg"
             alt="Pagina 278" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Weight Scaling Inference Rule</h4>
                <p>Un insight cheie (Hinton et al., 2012c): putem aproxima predictia ensemble-ului evaluand p(y|x) intr-un singur model - modelul cu toate unitatile, dar cu weights-urile inmultite cu probabilitatea de includere a unitatii respective. Motivatia: captureze expected value corect al input-ului fiecarei unitati.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Intuitia</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <p>La training cu p=0.5, fiecare unitate hidden este prezenta doar jumatate din timp. La test, toate unitatile sunt active, deci input-ul primit de unitatile urmatoare este ~2x mai mare! Solutia: inmultim weights cu 0.5.</p>
                            <div style="margin-top: 15px; display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                                <div style="background: var(--bg-lighter); padding: 15px; border-radius: 8px;">
                                    <strong>La Training</strong>
                                    <p style="margin-top: 10px;">E[h] = p Â· h_activ + (1-p) Â· 0</p>
                                    <p style="margin-top: 5px;">= p Â· h_activ</p>
                                </div>
                                <div style="background: var(--bg-lighter); padding: 15px; border-radius: 8px;">
                                    <strong>La Test (fara scaling)</strong>
                                    <p style="margin-top: 10px;">h = h_activ</p>
                                    <p style="margin-top: 5px; color: var(--warning);">Prea mare!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Cod: Doua Metode</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
# Metoda 1: Scale at test time (original)
# Training: h = h * mask (unde mask are p elemente = 1)
# Test: h = h * p

# Metoda 2: Inverted dropout (PyTorch default)
# Training: h = h * mask / p  (scalam deja la train)
# Test: h = h (nu facem nimic)

class InvertedDropout(nn.Module):
    def __init__(self, p=0.5):
        self.p = p

    def forward(self, x):
        if self.training:
            mask = (torch.rand_like(x) > self.p).float()
            return x * mask / (1 - self.p)  # Scale here!
        return x  # Nothing at test time
                            </div>
                            <p style="margin-top: 15px; color: var(--text-secondary);">Inverted dropout e preferat deoarece codul de test e mai simplu (nu necesita modificari).</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Pentru ce Modele e Exact?</h4>
                <p>Pentru modele liniare fara neliniaritati, weight scaling inference rule este <strong>exact</strong>. Considera un softmax classifier: P(y|v) = softmax(WâŠ¤v + b). Daca inmultim input-ul v element-wise cu un vector binar d, outputul sub-modelului este softmax(WâŠ¤(d âŠ™ v) + b).</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>Cazul Non-liniar</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="key-concept">
                                <p>Pentru retele cu neliniaritati (deep networks), weight scaling e doar o <strong>aproximare</strong>. Nu exista inca argumente teoretice pentru acuratetea acestei aproximari in deep nonlinear networks, dar <strong>empiric functioneaza foarte bine</strong>!</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>
