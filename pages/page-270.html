<section class="page-section" id="page-270">
    <div class="page-header">
        <div class="page-number">270</div>
        <div class="page-title">
            <h3>7.10 Regularizarea Reprezentarilor</h3>
            <span>Penalizari pe Activari</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/page-270.jpg"
             alt="Pagina 270" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Penalizare L¬π pe Reprezentare</h4>
                <p>Regularizarea reprezentarilor se face prin adaugarea unei penalizari pe activarile h (reprezentarea) la loss function:</p>
                <div class="formula" style="margin: 15px 0; padding: 15px; background: var(--bg-dark); border-radius: 8px; text-align: center;">
                    JÃÉ(Œ∏; X, y) = J(Œ∏; X, y) + Œ±Œ©(h)
                </div>
                <p>Cu Œ©(h) = ||h||‚ÇÅ = Œ£·µ¢|h·µ¢|, obtinem sparsitate reprezentationala.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">üéÆ</div>
                        <span>Cod: Sparse Activations</span>
                        <span class="arrow">‚ñ∂</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
class SparseAutoencoder(nn.Module):
    def __init__(self):
        self.encoder = nn.Linear(784, 256)
        self.decoder = nn.Linear(256, 784)

    def forward(self, x):
        h = torch.relu(self.encoder(x))  # Reprezentare
        x_recon = self.decoder(h)
        return x_recon, h

# Training cu sparsity penalty
model = SparseAutoencoder()
alpha = 0.001  # sparsity coefficient

for x in data_loader:
    x_recon, h = model(x)
    recon_loss = F.mse_loss(x_recon, x)
    sparsity_loss = h.abs().mean()  # L1 pe activari

    loss = recon_loss + alpha * sparsity_loss
    loss.backward()
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Alte Penalizari pentru Sparsitate</h4>
                <p>L¬π nu e singura optiune pentru a induce sparsitate. Alte variante includ:</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">üìö</div>
                        <span>Metode Alternative</span>
                        <span class="arrow">‚ñ∂</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <ul style="list-style: none; padding: 0;">
                                <li style="padding: 10px; background: var(--bg-lighter); margin-bottom: 10px; border-radius: 8px;">
                                    <strong>Student-t prior</strong> (Olshausen & Field, 1996; Bergstra, 2011)
                                    <p style="font-size: 0.85rem; color: var(--text-secondary);">Derivat din distributia Student-t pe reprezentare</p>
                                </li>
                                <li style="padding: 10px; background: var(--bg-lighter); margin-bottom: 10px; border-radius: 8px;">
                                    <strong>KL Divergence</strong> (Larochelle & Bengio, 2008)
                                    <p style="font-size: 0.85rem; color: var(--text-secondary);">Penalizeaza divergenta de la o activare target (ex: 0.01)</p>
                                </li>
                                <li style="padding: 10px; background: var(--bg-lighter); border-radius: 8px;">
                                    <strong>Average activation constraint</strong> (Lee et al., 2008; Goodfellow et al., 2009)
                                    <p style="font-size: 0.85rem; color: var(--text-secondary);">Forteaza media activarilor sa fie aproape de un target (ex: 0.01)</p>
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Orthogonal Matching Pursuit (OMP)</h4>
                <p>Alte abordari obtin sparsitate prin constrangeri hard. OMP (Pati et al., 1993) encodeaza input-ul x cu o reprezentare h care rezolva:</p>
                <div class="formula" style="margin: 15px 0; padding: 15px; background: var(--bg-dark); border-radius: 8px; text-align: center;">
                    argmin<sub>h, ||h||‚ÇÄ < k</sub> ||x - Wh||¬≤
                </div>
                <p>Unde ||h||‚ÇÄ este numarul de elemente nenule. OMP-1 (k=1) s-a dovedit un feature extractor foarte eficient pentru arhitecturi deep (Coates & Ng, 2011).</p>
            </div>
        </div>

    </div>
</section>
