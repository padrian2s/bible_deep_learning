<section class="page-section" id="page-216">
    <div class="page-header">
        <div class="page-number">216</div>
        <div class="page-title">
            <h3>Capitolul 6: Deep Feedforward Networks</h3>
            <span>6.4.1 Interpretarea Adancimii si 6.4.2 Alte Consideratii Arhitecturale</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT)-page-216.jpg"
             alt="Pagina 216" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>De ce sa alegem un Model Profund?</h4>
                <p>Ori de cate ori alegem un algoritm specific de machine learning, exprimam implicit anumite credinte prealabile despre ce fel de functie ar trebui sa invete algoritmul. Alegand un model profund, codificam o credinta foarte generala ca functia pe care vrem sa o invatam implica compunerea mai multor functii mai simple. Aceasta poate fi interpretata din perspectiva invatarii reprezentarilor.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">‚ú®</div>
                        <span>Doua Interpretari ale Adancimii</span>
                        <span class="arrow">‚ñ∂</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                                <div style="background: var(--bg-lighter); padding: 15px; border-radius: 8px;">
                                    <strong style="color: var(--primary);">1. Representation Learning</strong>
                                    <p style="font-size: 0.9rem; color: var(--text-secondary); margin-top: 10px;">Problema de invatare = descoperirea factorilor de variatie care pot fi descrisi in termenii altor factori mai simpli.</p>
                                    <p style="font-size: 0.85rem; margin-top: 5px;">Ex: Imagini ‚Üí Margini ‚Üí Forme ‚Üí Obiecte</p>
                                </div>
                                <div style="background: var(--bg-lighter); padding: 15px; border-radius: 8px;">
                                    <strong style="color: var(--accent);">2. Computational</strong>
                                    <p style="font-size: 0.9rem; color: var(--text-secondary); margin-top: 10px;">Functia tinta = un program cu mai multi pasi, fiecare pas foloseste output-ul pasului anterior.</p>
                                    <p style="font-size: 0.85rem; margin-top: 5px;">Output-urile intermediare = "pointeri" interni</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Dovezi Empirice pentru Beneficiile Adancimii</h4>
                <p>Empiric, adancimea mai mare pare sa rezulte intr-o generalizare mai buna pentru o varietate larga de task-uri (Bengio et al., 2007; Erhan et al., 2009; Bengio, 2009; Mesnil et al., 2011; Krizhevsky et al., 2012; si multe altele). Aceasta sugereaza ca folosirea arhitecturilor profunde exprima intr-adevar un prior util peste spatiul functiilor pe care modelul le invata.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">üìö</div>
                        <span>Studii de Caz</span>
                        <span class="arrow">‚ñ∂</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <ul class="reference-list">
                                <li class="reference-item">
                                    <span>üñºÔ∏è</span>
                                    <div><strong>ImageNet 2012</strong> - AlexNet (8 straturi) a revolutionat computer vision</div>
                                </li>
                                <li class="reference-item">
                                    <span>üèÜ</span>
                                    <div><strong>ImageNet 2014</strong> - VGGNet (19 straturi), GoogLeNet (22 straturi)</div>
                                </li>
                                <li class="reference-item">
                                    <span>üìà</span>
                                    <div><strong>ImageNet 2015</strong> - ResNet (152 straturi!) - demonstrand ca adancimea functioneaza</div>
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>6.4.2 Alte Consideratii Arhitecturale</h4>
                <p>Pana acum am descris retelele neuronale ca fiind simple lanturi de straturi. In practica, retelele neuronale arata o diversitate considerabil mai mare. Multe arhitecturi au fost dezvoltate pentru task-uri specifice: retele convolutionale pentru viziune computationala (Capitolul 9), retele recurente pentru procesarea secventelor (Capitolul 10).</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">üéÆ</div>
                        <span>Dincolo de Lanturile Simple</span>
                        <span class="arrow">‚ñ∂</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import torch.nn as nn

# Arhitecturi NON-secventiale:

# 1. Skip Connections (ResNet)
class ResidualBlock(nn.Module):
    def forward(self, x):
        return x + self.conv_block(x)  # Skip!

# 2. Dense Connections (DenseNet)
# Fiecare strat primeste output-ul TUTUROR straturilor anterioare

# 3. Multi-branch (Inception/GoogLeNet)
# Mai multe "cai" paralele cu filtre de dimensiuni diferite

# 4. Attention mechanisms (Transformers)
# Conexiuni dinamice bazate pe continut

# Skip connections ajuta gradientul sa curga!
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>
