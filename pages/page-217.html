<section class="page-section" id="page-217">
    <div class="page-header">
        <div class="page-number">217</div>
        <div class="page-title">
            <h3>Capitolul 6: Deep Feedforward Networks</h3>
            <span>6.4.2 Figura 6.6 si Conexiuni intre Straturi</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/page-217.jpg"
             alt="Pagina 217" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Figura 6.6: Dovezi Empirice - Adancimea Imbunatateste Generalizarea</h4>
                <p>Aceasta figura arata rezultate empirice demonstrand ca retelele mai profunde generalizeaza mai bine cand sunt folosite pentru transcrierea numerelor multi-digit din fotografii de adrese. Datele provin de la Goodfellow et al. (2014d). Acuratetea pe test set creste consistent cu cresterea adancimii, de la ~93.5% cu 3 straturi la ~96% cu 11 straturi.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Interpretarea Graficului</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="background: var(--bg-lighter); padding: 20px; border-radius: 12px;">
                                <table style="width: 100%; border-collapse: collapse; font-size: 0.9rem;">
                                    <tr style="background: var(--primary);">
                                        <th style="padding: 10px;">Adancime</th>
                                        <th style="padding: 10px;">Acuratete Test</th>
                                        <th style="padding: 10px;">Imbunatatire</th>
                                    </tr>
                                    <tr style="background: var(--bg-dark);">
                                        <td style="padding: 10px;">3 straturi</td>
                                        <td style="padding: 10px;">~93.5%</td>
                                        <td style="padding: 10px;">baseline</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 10px;">5 straturi</td>
                                        <td style="padding: 10px;">~94.5%</td>
                                        <td style="padding: 10px;">+1.0%</td>
                                    </tr>
                                    <tr style="background: var(--bg-dark);">
                                        <td style="padding: 10px;">7 straturi</td>
                                        <td style="padding: 10px;">~95.2%</td>
                                        <td style="padding: 10px;">+1.7%</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 10px;">11 straturi</td>
                                        <td style="padding: 10px; color: var(--success);">~96.0%</td>
                                        <td style="padding: 10px; color: var(--success);">+2.5%</td>
                                    </tr>
                                </table>
                                <p style="margin-top: 15px; color: var(--text-secondary); font-size: 0.9rem;">Nota: Alte mariri ale modelului (latime) nu au acelasi efect benefic!</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Tipuri de Conexiuni intre Straturi</h4>
                <p>O alta consideratie cheie de design este exact cum sa conectam straturile. In stratul neural implicit descris prin transformarea liniara via matricea W, fiecare unitate de input este conectata la fiecare unitate de output. Multe retele specializate au mai putine conexiuni, astfel incat fiecare unitate din stratul de input este conectata doar la un mic subset de unitati din stratul de output.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Tipuri de Conectivitate</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
import torch.nn as nn

# 1. Fully Connected (Dense)
# Fiecare input -> fiecare output
fc = nn.Linear(100, 50)  # 100*50 = 5000 conexiuni

# 2. Convolutional (Sparse, shared weights)
# Fiecare output <- patch local de input
conv = nn.Conv2d(3, 64, kernel_size=3)
# 3*3*3*64 = 1728 parametri (mult mai putin!)

# 3. Locally Connected (Sparse, unshared)
# Ca convolutia dar fara weight sharing

# 4. Skip Connections
# Conexiuni care "sar" peste straturi
# x + F(x) in loc de doar F(x)

# 5. Attention (Dynamic connections)
# Conexiunile sunt calculate din date
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Skip Connections: Gradiente Mai Usoare</h4>
                <p>In general, straturile nu trebuie sa fie conectate in lant, desi aceasta este practica cea mai comuna. Multe arhitecturi construiesc un lant principal dar apoi adauga caracteristici arhitecturale suplimentare, cum ar fi skip connections mergand de la stratul i la stratul i+2 sau mai departe. Aceste skip connections faciliteaza curgerea gradientului de la straturile de output catre straturile mai aproape de input.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>ResNet: Revolutia Skip Connections</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="key-concept">
                                <strong>ResNet (He et al., 2015):</strong>
                                <p style="margin-top: 10px;">In loc de a invata H(x), invatam F(x) = H(x) - x, astfel output = x + F(x). Daca F optimal e aproape de 0, e mai usor de invatat decat identitatea!</p>
                            </div>
                            <div class="code-block" style="margin-top: 15px;">
# Residual Block
class ResBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.relu = nn.ReLU()

    def forward(self, x):
        residual = x  # Skip connection
        out = self.relu(self.conv1(x))
        out = self.conv2(out)
        out += residual  # x + F(x)
        return self.relu(out)
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>
