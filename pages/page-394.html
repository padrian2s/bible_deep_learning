<section class="page-section" id="page-394">
    <div class="page-header">
        <div class="page-number">394</div>
        <div class="page-title">
            <h3>Universalitatea RNN si Turing Machines</h3>
            <span>Puterea Computationala a RNN-urilor</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT)-page-394.jpg"
             alt="Pagina 394" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>RNN-urile sunt Turing Complete!</h4>
                <p>Un rezultat teoretic remarcabil: RNN-ul din Figura 10.3 este <strong>universal</strong> - poate calcula ORICE functie computabila de o masina Turing! Aceasta putere vine din recurenta hidden-to-hidden care permite memorarea si manipularea informatiei pe termen lung.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>Dovezi Teoretice</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <ul class="reference-list">
                                <li class="reference-item">
                                    <span>ðŸ“–</span>
                                    <div><strong>Siegelmann & Sontag, 1991, 1995</strong> - Demonstratie ca RNN poate simula o masina Turing</div>
                                </li>
                                <li class="reference-item">
                                    <span>ðŸ“–</span>
                                    <div><strong>Hyotyniemi, 1996</strong> - Analiza complexitatii computationale</div>
                                </li>
                            </ul>
                            <div class="key-concept" style="margin-top: 15px;">
                                <h5>Rezultat cheie:</h5>
                                <p>Un RNN cu doar 886 de unitati poate simula o masina Turing universala!</p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Ce inseamna practic?</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="display: grid; gap: 10px;">
                                <div style="background: linear-gradient(90deg, rgba(16, 185, 129, 0.25) 0%, var(--bg-lighter) 20%); padding: 12px; border-radius: 8px; box-shadow: 0 0 15px rgba(16, 185, 129, 0.1);">
                                    <strong style="color: var(--success);">Teoretic:</strong>
                                    <span style="color: var(--text-secondary);"> RNN poate invata ORICE algoritm</span>
                                </div>
                                <div style="background: linear-gradient(90deg, rgba(245, 158, 11, 0.25) 0%, var(--bg-lighter) 20%); padding: 12px; border-radius: 8px; box-shadow: 0 0 15px rgba(245, 158, 11, 0.1);">
                                    <strong style="color: var(--warning);">Practic:</strong>
                                    <span style="color: var(--text-secondary);"> Antrenarea pentru a gasi acesti parametri este extrem de dificila</span>
                                </div>
                                <div style="background: linear-gradient(90deg, rgba(6, 182, 212, 0.25) 0%, var(--bg-lighter) 20%); padding: 12px; border-radius: 8px; box-shadow: 0 0 15px rgba(6, 182, 212, 0.1);">
                                    <strong style="color: var(--accent);">Concluzie:</strong>
                                    <span style="color: var(--text-secondary);"> Puterea expresiva nu e problema - optimizarea este!</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Ecuatiile Forward Propagation</h4>
                <p>Pentru RNN-ul din Figura 10.3, forward propagation implica calcularea secventiala a starilor h si output-urilor o. Incepem cu o stare initiala h<sup>(0)</sup> si aplicam functia de activare (de obicei tanh) la fiecare pas.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Ecuatiile 10.8-10.11</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="background: var(--bg-lighter); padding: 15px; border-radius: 8px;">
                                <div class="formula" style="margin: 10px 0;">
                                    a<sup>(t)</sup> = b + WÂ·h<sup>(t-1)</sup> + UÂ·x<sup>(t)</sup>
                                    <span style="color: var(--text-secondary); font-size: 0.85rem;"> (10.8)</span>
                                </div>
                                <div class="formula" style="margin: 10px 0;">
                                    h<sup>(t)</sup> = tanh(a<sup>(t)</sup>)
                                    <span style="color: var(--text-secondary); font-size: 0.85rem;"> (10.9)</span>
                                </div>
                                <div class="formula" style="margin: 10px 0;">
                                    o<sup>(t)</sup> = c + VÂ·h<sup>(t)</sup>
                                    <span style="color: var(--text-secondary); font-size: 0.85rem;"> (10.10)</span>
                                </div>
                                <div class="formula" style="margin: 10px 0;">
                                    Å·<sup>(t)</sup> = softmax(o<sup>(t)</sup>)
                                    <span style="color: var(--text-secondary); font-size: 0.85rem;"> (10.11)</span>
                                </div>
                            </div>
                            <p style="margin-top: 15px; color: var(--text-secondary); font-size: 0.9rem;">
                                <strong>Parametri:</strong> b, c = bias; U = input weights; W = recurrent weights; V = output weights
                            </p>
                        </div>
                    </div>
                </div>
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Cod: Forward Pass Complet</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
                                <pre>import torch
import torch.nn as nn
import torch.nn.functional as F

class RNNFromScratch(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        # Parametri ca in ecuatii
        self.U = nn.Linear(input_size, hidden_size)   # input -> hidden
        self.W = nn.Linear(hidden_size, hidden_size)  # hidden -> hidden
        self.V = nn.Linear(hidden_size, output_size)  # hidden -> output

    def forward(self, x_sequence, h0):
        outputs = []
        h = h0

        for x_t in x_sequence:
            # Eq 10.8-10.9: h(t) = tanh(b + W*h(t-1) + U*x(t))
            a = self.W(h) + self.U(x_t)
            h = torch.tanh(a)

            # Eq 10.10-10.11: y(t) = softmax(c + V*h(t))
            o = self.V(h)
            y = F.softmax(o, dim=-1)
            outputs.append(y)

        return torch.stack(outputs), h</pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>
