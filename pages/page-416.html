<section class="page-section" id="page-416">
    <div class="page-header">
        <div class="page-number">416</div>
        <div class="page-title">
            <h3>10.7 The Challenge of Long-Term Dependencies</h3>
            <span>Provocarea Dependentelor pe Termen Lung</span>
        </div>
    </div>
    <div class="image-container">
        <img src="../book_page_jpg/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT)-page-416.jpg"
             alt="Pagina 416" class="page-image" onclick="zoomImage(this)">
    </div>
    <div class="explanation-content">

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>Problema Fundamentala a RNN-urilor</h4>
                <p>Provocarea matematica centrala in antrenarea RNN-urilor: <strong>gradientii tind sa dispara sau sa explodeze</strong> cand sunt propagati inapoi prin multi pasi temporali. Aceasta face invatarea dependentelor pe termen lung extrem de dificila.</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon animation">âœ¨</div>
                        <span>Vanishing vs Exploding</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                                <div style="background: linear-gradient(90deg, rgba(245, 158, 11, 0.25) 0%, var(--bg-lighter) 20%); padding: 15px; border-radius: 8px; box-shadow: 0 0 15px rgba(245, 158, 11, 0.1);">
                                    <strong style="color: var(--warning);">Vanishing Gradients</strong>
                                    <p style="color: var(--text-secondary); margin-top: 10px; font-size: 0.9rem;">
                                        Gradientii devin exponential mici â†’ modelul nu invata dependentele lungi
                                    </p>
                                    <p style="font-family: monospace; margin-top: 10px;">|âˆ‚h<sup>(t)</sup>/âˆ‚h<sup>(1)</sup>| â†’ 0</p>
                                </div>
                                <div style="background: linear-gradient(90deg, rgba(6, 182, 212, 0.25) 0%, var(--bg-lighter) 20%); padding: 15px; border-radius: 8px; box-shadow: 0 0 15px rgba(6, 182, 212, 0.1);">
                                    <strong style="color: var(--accent);">Exploding Gradients</strong>
                                    <p style="color: var(--text-secondary); margin-top: 10px; font-size: 0.9rem;">
                                        Gradientii devin exponential mari â†’ antrenarea devine instabila
                                    </p>
                                    <p style="font-family: monospace; margin-top: 10px;">|âˆ‚h<sup>(t)</sup>/âˆ‚h<sup>(1)</sup>| â†’ âˆž</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon reference">ðŸ“š</div>
                        <span>Referinte Fundamentale</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <ul class="reference-list">
                                <li class="reference-item"><span>ðŸ“–</span><div><strong>Hochreiter, 1991</strong> - Prima analiza a vanishing gradients</div></li>
                                <li class="reference-item"><span>ðŸ“–</span><div><strong>Bengio et al., 1994</strong> - Analiza detaliata</div></li>
                                <li class="reference-item"><span>ðŸ“–</span><div><strong>Doya, 1993</strong> - Conditii pentru exploding</div></li>
                                <li class="reference-item"><span>ðŸ“–</span><div><strong>Pascanu et al., 2013</strong> - Tratament modern</div></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="interactive-paragraph">
            <div class="paragraph-main" onclick="toggleParagraph(this)">
                <h4>De ce se intampla?</h4>
                <p>RNN-urile implica <strong>compunerea</strong> aceleiasi functii de mai multe ori. Multiplicarea repetata a Jacobienilor poate duce la crestere/descrestere exponentiala. E similar cu de ce 0.9<sup>100</sup> â‰ˆ 0 si 1.1<sup>100</sup> â‰ˆ 13780!</p>
            </div>
            <div class="expandable-sections">
                <div class="section-tab">
                    <div class="section-header" onclick="toggleSection(this)">
                        <div class="section-icon simulation">ðŸŽ®</div>
                        <span>Analogie Simpla</span>
                        <span class="arrow">â–¶</span>
                    </div>
                    <div class="section-content">
                        <div class="section-body">
                            <div class="code-block">
                                <pre># Vanishing: multiplicare repetata cu valoare < 1
>>> 0.9 ** 100
2.656e-05  # aproape 0!

# Exploding: multiplicare repetata cu valoare > 1
>>> 1.1 ** 100
13780.61  # explozie!

# In RNN, "valoarea" este norma Jacobianului W</pre>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>
